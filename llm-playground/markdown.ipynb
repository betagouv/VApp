{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Il s'agit d'un retour sur les premiers développements de VApp, une application qui tente de répondre, à l'aide de la GenAI, au besoin d'identifier rapidement et sans grandes compétences techniques l'éligibilité des aides pour un projet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raison de certains choix technique\n",
    "\n",
    "Le code associé à chaque partie sera détaillé plus bas dans le document Markdown/Jupyter.\n",
    "\n",
    "Nous nous appuyons principalement sur l'API d'Aide Territoire pour constituer la base de données d'maide. Cette API représente en effet une première source fiable et régulièrement mise à jour concernant les subventions pour les collectivités.\n",
    "\n",
    "Nous avons également évoqué la possibilité d'interroger les formulaires d'aide de Démarches Simplifiées, mais cela présente un défi de taille. En effet, toutes les aides ne sont pas centralisées sur Démarches Simplifiées.\n",
    "\n",
    "J'ai imaginé l'application autour de trois composantes principales :\n",
    "- Frontend\n",
    "- Backend\n",
    "- Serveur GPU dédié à la partie LLM et à l'IA de manière plus générale\n",
    "Je vais détailler le choix de chacune de ces solutions par la suite. Pour résumé :\n",
    "\n",
    "Dans le cadre de la création d'une application web basée sur l'intelligence artificielle, j'ai choisi de structurer l'architecture de manière modulaire en séparant clairement le frontend, le backend et un serveur GPU dédié aux tâches liées à l'IA. L'objectif principal derrière cette approche est d'optimiser les performances et de garantir une meilleure gestion des ressources.\n",
    "\n",
    "En effet, les GPU sont essentiels pour accélérer les calculs massivement parallèles, caractéristiques des modèles d'apprentissage automatique et des réseaux neuronaux complexes. Ces derniers nécessitent de traiter de grandes quantités de données en simultané, ce qui rend les GPU bien plus efficaces que les CPU pour ces types de tâches. En déployant un serveur spécifique aux calculs IA, je m'assure que l'inférence et l'entraînement des modèles de type LLM se déroulent de manière fluide, sans ralentir le reste de l'application.\n",
    "\n",
    "Ce découplage permet également d'améliorer la scalabilité. Si les besoins en calcul augmentent, il est possible d'ajouter davantage de ressources GPU sans impacter la performance du frontend ou du backend. Cela réduit les risques de goulots d'étranglement et offre une meilleure flexibilité en termes d'évolutivité. Par ailleurs, cette architecture permet de mieux contrôler les coûts en allouant des ressources puissantes là où elles sont vraiment nécessaires, tout en maintenant une infrastructure classique pour les autres composants de l'application​.\n",
    "\n",
    "En conclusion, cette séparation claire entre les différentes couches de l'application et l'utilisation d'un serveur dédié aux calculs IA garantit non seulement des performances optimales, mais aussi une flexibilité et une évolutivité accrues, essentielles pour l'évolution future du projet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frontend\n",
    "Le choix du frontend a été relativement simple. Je travaillais initialement avec Python Flask en backend, et j'utilisais directement le combo HTML/CSS/JS. Ensuite, j'ai voulu implémenter des fonctionnalités plus complexes et plus dynamiques, et j'ai hésité entre Angular et React.\n",
    "\n",
    "La communauté beta.gouv utilisant principalement React et proposant déjà un template React DFSR, mon choix s'est orienté vers cette solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backend\n",
    "Initialement sur Flask, je suis rapidement passé à Django. Django est un framework Python avec une communauté déjà importante chez beta.gouv. De plus, il semblait plus adapté pour créer un backend sous forme d'API, ce qui permettrait une intégration plus simple à long terme avec Aide Territoire ou Mon Espace Collectivité, selon moi.\n",
    "\n",
    "Je souhaitais également rester sur un framework Python, car cela simplifie grandement les interactions avec les LLM, quelle que soit l'approche adoptée par la suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serveur GPU - IA\n",
    "Pour le serveur GPU IA j'ai préféré partir sur une solution très low level, pour plusieurs raison :\n",
    "- Des application comme ollama permette une interaction et un déploiement extraimeent simple sous forme d'API à un large panel de LLM\n",
    "- Permet de facilement avoir accès à de la ressoure GPU qui est reconnue pour faciliter l'usage de large réseau neuronaux.\n",
    "- Pour le moment le problème va être brute force à l'aide de LLM mais il sera envisagble de a termes créer des réseau de neurone dédié à la tache beaucoup plus éfficace et ne nécessitant pas de autant de ressoure. Le serveur dédié facilitera le protoypage de ces solutions.\n",
    "- Dans notre contexte nous aurions pu utiliser des api tel que celle d'open IA ou de mistral, cependant nous voulons pouvoir tester rapidement une large variété de model open source pour trouver celui le plus adapter à la tache, quit à employer également une compinéaison d'autre méthode. Nous avons également pour ojbectif de montrer la faisablité d'utiliser des modèles customes sur des serveur spécialisé pour mettre en évidence où non la néssésité à plus long terme de se dauter de serveur dédié GenIA au seins des ministères.\n",
    "\n",
    "### Usage de la solution Ollama\n",
    "Je souhaite utiliser Ollama (https://ollama.com/) pour la gestion des LLM, car il permet une gestion extrêmement simplifiée des modèles via une API, tout en offrant la possibilité d'ajuster très finement les modèles pour des utilisations spécialisées.\n",
    "\n",
    "Il intègre également plusieurs méthodes d'optimisation, notamment la quantization, ce qui permet de réduire considérablement l'utilisation des ressources serveur.\n",
    "\n",
    "Enfin, je pense qu'il n'est pas nécessaire de nous concentrer sur l'optimisation ultime des appels aux LLM pour le moment, car cela pourra être pris en compte plus tard dans une application à plus grande échelle, produite par l'État. De plus, nous pouvons facilement faire évoluer les instances d'Ollama directement via le backend de notre application (par exemple, en prévoyant une gestion de la charge qui ouvre ou ferme des instances Ollama en fonction des besoins)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecte des données d'aides territoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des libraires\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation d'une classe pour dialoguer plus simplement avec l'API de AT\n",
    "class APIClient:\n",
    "    def __init__(self,x_auth_token):\n",
    "        self.x_auth_token = x_auth_token\n",
    "        self.base_uri = self.get_api_base_url()\n",
    "        self.bearer_token = self.get_bearer_token()\n",
    "        self.session = self.create_session() \n",
    "\n",
    "    def get_api_base_url(self):\n",
    "        return 'https://aides-territoires.beta.gouv.fr/api/'\n",
    "\n",
    "    def get_bearer_token(self):\n",
    "        url = 'https://aides-territoires.beta.gouv.fr/api/connexion/'\n",
    "        headers = {\n",
    "            'accept': 'application/json',\n",
    "            'Content-Type': 'application/json',\n",
    "            'X-AUTH-TOKEN': self.x_auth_token\n",
    "        }\n",
    "        response = requests.post(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.json()['token']\n",
    "\n",
    "    def create_session(self):\n",
    "        session = requests.Session()\n",
    "        session.headers.update({\n",
    "            'Authorization': f'Bearer {self.bearer_token}',\n",
    "            'Accept': 'application/json'\n",
    "        })\n",
    "        return session\n",
    "\n",
    "    def simple_request_endpoint(self, endpoint, method='GET'):\n",
    "        url = f'{self.base_uri}{endpoint}'\n",
    "        response = self.session.request(method, url)\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "\n",
    "    def simple_request_url(self, url, method='GET'):\n",
    "        response = self.session.request(method, url)\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "    \n",
    "# Utilisation vous avez besoin d'un x_auth_token fournit par aide territoires\n",
    "client = APIClient(x_auth_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je collecte l'ensemble des données de toutes les aides d'Aide Territoire (AT) que je stocke dans une liste, afin de pouvoir ensuite les convertir en Pandas DataFrame et les enregistrer en local. Je procède ainsi pour plusieurs raisons (tout en sachant pertinemment que l'idéal serait d'avoir notre propre base de données) :\n",
    "\n",
    "- La base de données n'est pas très volumineuse, environ 50 Mo tout au plus.\n",
    "- Le LLM représente la plus grande part du temps de réponse. Cependant, durant la phase de prototypage, nous préférons éviter les problèmes de latence et l'instabilité potentielle liée à l'API d'AT.\n",
    "- Stocker les données dans un Pandas DataFrame permet de visualiser rapidement et de manière exhaustive le contenu de la base de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.simple_request_endpoint('aids/?page=1&itemsPerPage=30')\n",
    "data = response.json()\n",
    "data_collect = data['results']\n",
    "error_counter = 0\n",
    "while data['next'] or error_counter >10:\n",
    "    try:\n",
    "        url = data['next']\n",
    "        print(url)\n",
    "        response = client.simple_request_url(url)\n",
    "        data = response.json()\n",
    "        data_collect = data_collect + data['results']\n",
    "        error_counter = 0\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        error_counter += 1\n",
    "\n",
    "data_at = pd.DataFrame(data_collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'utilisation de la base de données, nous avons remarqué qu'un grand nombre d'aides contiennent moins de 3 000 caractères (environ 500 mots). Cela représente 90 % de la base, mais pour garantir l'efficacité de l'outil, nous avons préféré les exclure et ne conserver que les aides les mieux décrites, soit un total de 400."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_at_select = data_at[['id','name','description','eligibility','project_examples']].dropna(subset='description')\n",
    "\n",
    "sub_min_description = 3000\n",
    "\n",
    "data_at_select = data_at_select[data_at_select['description'].apply(len)>sub_min_description].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage du LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage 1 - Attribution d'un score en fonction d'une aide et de la description d'un projet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, nous sommes grandement contraints par les limites techniques propres aux LLM. Voici les principales raisons de mon approche :\n",
    "\n",
    "- Les aides contiennent entre 500 et 10 000 mots, soit environ 250 à 5 000 tokens. Les LLM ayant une capacité limitée à gérer le contexte (et plus le contexte est long, moins ils sont fiables), j'ai préféré limiter chaque appel à une seule aide et à une description de projet.\n",
    "- Cette approche est similaire à d'autres méthodes déjà connues avec les LLM.\n",
    "- Le RAG (Retrieval-Augmented Generation) a été écarté pour : UNE VRAIE RAISON À AJOUTER.\n",
    "- Nous bouclons sur les réponses car... EXPLICATION À AJOUTER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LE CODE à ajouter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage 2 - Questionner sur une aide pour en améliorer l'éligibilité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN CONSTRUCTION"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
