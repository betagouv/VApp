# Nature du dossier
Ce dossier a pour objectif d'expliquer les choix techniques en matière d'IA (LLM) et de servir de terrain d'expérimentation pour le prototypage sur cet outil.

Nous nous basons principalement sur Ollama (https://hub.docker.com/r/ollama/ollama) pour les appels aux LLM. Vous pouvez l'installer simplement via la commande suivante :

*Installation CPU uniquement :*
```bash
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

*Installation GPU (NVIDIA) :*
```bash
docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```
Le modèle LLM et sa version définitive ne sont pas encore arrêtés.

Vous pouvez installer l'environnement de test via la commande suivante (après avoir installé un environnement Python) :
```bash
pip install -r requirements.txt
```